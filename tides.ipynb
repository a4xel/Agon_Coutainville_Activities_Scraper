{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Do the necessary imports\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Scrape a first time to all the months where data about the tides can be found and create a list that \n",
    "can be pluged in the url to load all the pages where scrapable data is available\"\"\"\n",
    "\n",
    "url = \"https://marine.meteoconsult.fr/meteo-marine/horaires-des-marees/pointe-d-agon-944/juillet-2022\"\n",
    "\n",
    "response=requests.get(url)\n",
    "\n",
    "soup1 = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "list_dates=[]\n",
    "list_years=[]\n",
    "\n",
    "for i in soup1.find_all(class_=\"month-container\"):\n",
    "    list_dates.append((i.find(class_=\"name\").text))\n",
    "    list_years.append((i.find(class_=\"year\").text))\n",
    "    \n",
    "list_dates_and_years = [m+str(n) for m,n in zip(list_dates, list_years)]\n",
    "\n",
    "month_years_final=[]\n",
    "for i in list_dates_and_years:\n",
    "    i=i.replace(\"20\", \"-20\")\n",
    "    month_years_final.append(i)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loop on each month and extract the information\"\"\"\n",
    "\n",
    "url = \"https://marine.meteoconsult.fr/meteo-marine/horaires-des-marees/pointe-d-agon-944\"\n",
    "\n",
    "\n",
    "for month in month_years_final:\n",
    "    \n",
    "    days=[]\n",
    "    high_tide_time=[]\n",
    "    low_tide_time=[]\n",
    "    first_tide_coeff=[]\n",
    "    second_tide_coeff=[]\n",
    "      \n",
    "    req=requests.get(url + f\"/{month}\")\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    \n",
    "    for i in soup.find_all(class_=\"tide-date week\"):\n",
    "        days.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all(class_=\"high-tide\"):\n",
    "        high_tide_time.append(i.find(class_=\"hour\").text)\n",
    "    \n",
    "    for i in soup.find_all(class_=\"low-tide\"):\n",
    "        low_tide_time.append(i.find(class_=\"hour\").text)\n",
    "        \n",
    "    for index,value in enumerate(soup.find_all(class_=\"coef tide-coef-level-2\")):\n",
    "        if (index%2) == 0:\n",
    "            first_tide_coeff.append(value.text)\n",
    "        else:\n",
    "            second_tide_coeff.append(value.text)\n",
    "            \n",
    "    \n",
    "    \"\"\"Do the necessary reshapes\"\"\"\n",
    "    \n",
    "    days_formated=[]\n",
    "    for i in days:\n",
    "        i=i.replace(\"\\n\",\"\")\n",
    "        days_formated.append(i)\n",
    "    \n",
    "    first_tide_coeff_formated=[]\n",
    "    for i in first_tide_coeff:\n",
    "        i=i.replace(\"\\n\",\"\")\n",
    "        first_tide_coeff_formated.append(i)\n",
    "    \n",
    "    second_tide_coeff_formated=[]\n",
    "    for i in second_tide_coeff:\n",
    "        i=i.replace(\"\\n\",\"\")\n",
    "        second_tide_coeff_formated.append(i)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "days_week=[]\n",
    "days_weekend=[]\n",
    "high_tide_time=[]\n",
    "low_tide_time=[]\n",
    "first_tide_coeff=[]\n",
    "second_tide_coeff=[]\n",
    "  \n",
    "\n",
    "\n",
    "for i in soup1.find_all(class_=\"tide-date week\"):\n",
    "    days_week.append(i.text)\n",
    "    \n",
    "for i in soup1.find_all(class_=\"tide-date weekEnd\"):\n",
    "    days_weekend.append(i.text)\n",
    "\n",
    "for i in soup1.find_all(class_=\"high-tide\"):\n",
    "    high_tide_time.append(i.find(class_=\"hour\").text)\n",
    "\n",
    "for i in soup1.find_all(class_=\"low-tide\"):\n",
    "    low_tide_time.append(i.find(class_=\"hour\").text)\n",
    "\n",
    "for index,value in enumerate(soup1.find_all(class_=\"coef tide-coef-level-2\")):\n",
    "    if (index%2) == 0:\n",
    "        first_tide_coeff.append(value.text)\n",
    "    else:\n",
    "        second_tide_coeff.append(value.text)\n",
    "\n",
    "\n",
    "\"\"\"Do the necessary reshapes\"\"\"\n",
    "\n",
    "days_week_formated=[]\n",
    "for i in days_week:\n",
    "    i=i.replace(\"\\n\",\"\")\n",
    "    days_week_formated.append(i)\n",
    "    \n",
    "days_weekend_formated=[]\n",
    "for i in days_weekend:\n",
    "    i=i.replace(\"\\n\",\"\")\n",
    "    days_weekend_formated.append(i)\n",
    "\n",
    "first_tide_coeff_formated=[]\n",
    "for i in first_tide_coeff:\n",
    "    i=i.replace(\"\\n\",\"\")\n",
    "    first_tide_coeff_formated.append(i)\n",
    "\n",
    "second_tide_coeff_formated=[]\n",
    "for i in second_tide_coeff:\n",
    "    i=i.replace(\"\\n\",\"\")\n",
    "    second_tide_coeff_formated.append(i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_number=[]\n",
    "for i in days_week_formated:\n",
    "    day_number.append(i[-2:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 7',\n",
       " ' 8',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jeudi 7',\n",
       " 'vendredi 8',\n",
       " 'lundi 11',\n",
       " 'mardi 12',\n",
       " 'mercredi 13',\n",
       " 'jeudi 14',\n",
       " 'vendredi 15',\n",
       " 'lundi 18',\n",
       " 'mardi 19',\n",
       " 'mercredi 20',\n",
       " 'jeudi 21',\n",
       " 'vendredi 22',\n",
       " 'lundi 25',\n",
       " 'mardi 26',\n",
       " 'mercredi 27',\n",
       " 'jeudi 28',\n",
       " 'vendredi 29']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_week_formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b23a059732edb4bfbb23e8cd4fc458f3edfb684a0d6eec5d0f4069a80889721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
